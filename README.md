# ğŸ“š Virtual Debate Panel

A multi-perspective chat application that enables users to query multiple authors concurrently, with each author responding in their unique voice and highlighting intellectual disagreements.

## ğŸ¯ Project Overview

The Virtual Debate Panel uses a Retrieval-Augmented Generation (RAG) pipeline with semantic routing to automatically select 2-5 relevant authors to respond to user queries. Each author maintains their distinct voice, tone, and philosophical stance, creating a dynamic intellectual debate.

## âœ¨ Key Features

- **Intelligent Author Selection**: Semantic router automatically selects relevant authors based on query content
- **Concurrent Multi-Author Responses**: Parallel RAG pipeline for simultaneous author responses
- **Multi-Round Debates**: NEW! Make authors "fight" - authors respond to and critique each other's perspectives across multiple rounds
- **Distinct Author Voices**: Each author maintains unique tone, vocabulary, and philosophical stance
- **Comparative Formatting**: Clear presentation of contrasting viewpoints
- **Brief Responses**: Max 3 paragraphs per author for concise, focused debate

## ğŸ—ï¸ Architecture

### Three-Layer System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               API Layer (FastAPI)                   â”‚
â”‚  â€¢ REST endpoints for queries                       â”‚
â”‚  â€¢ WebSocket support for streaming                  â”‚
â”‚  â€¢ Authentication & rate limiting                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Logic Layer (Semantic Router)              â”‚
â”‚  â€¢ Query vectorization                              â”‚
â”‚  â€¢ Cosine similarity calculation                    â”‚
â”‚  â€¢ Author panel selection (threshold-based)         â”‚
â”‚  â€¢ Response aggregation & formatting                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Processing Layer (RAG Pipeline)               â”‚
â”‚  â€¢ Vector database queries (ChromaDB/Pinecone)      â”‚
â”‚  â€¢ LLM integration (Gemini 2.5 Pro / OpenAI)        â”‚
â”‚  â€¢ Parallel concurrent processing                   â”‚
â”‚  â€¢ System prompt enforcement                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Data Layer (The Library)                  â”‚
â”‚  â€¢ Vector database (embeddings)                     â”‚
â”‚  â€¢ Author expertise profiles                        â”‚
â”‚  â€¢ Book chunks & metadata                           â”‚
â”‚  â€¢ System prompts repository                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Poetry (recommended) or pip
- API keys for:
  - LLM provider (Google Gemini, OpenAI, or Anthropic)
  - Vector database (ChromaDB local or Pinecone cloud - ChromaDB is default)

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd virtual-debate-panel

# Install dependencies using Poetry
poetry install

# Set up environment variables
cp .env.example .env
# Edit .env with your API keys

# Initialize the vector database
poetry run python scripts/init_database.py

# Run data ingestion (Phase 1: Marx only)
poetry run python scripts/ingest_author.py --author marx --input data/raw/marx/
```

**Alternative: Using pip**
```bash
pip install -r requirements.txt
python scripts/init_database.py
```

### Running the Application

```bash
# Start the API server using Poetry
poetry run uvicorn src.api.main:app --reload --port 8000

# In a separate terminal, start the UI dev server
cd src/ui
python -m http.server 3000
```

Visit `http://localhost:3000` to access the chat interface.

## ğŸ“ Project Structure

```
virtual-debate-panel/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                    # Data layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ vector_db.py        # Vector database interface
â”‚   â”‚   â”œâ”€â”€ models.py           # Data models (Author, Query, Response)
â”‚   â”‚   â””â”€â”€ embeddings.py       # Embedding generation
â”‚   â”œâ”€â”€ processing/              # Processing layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_client.py       # LLM API integration
â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py     # RAG retrieval & generation
â”‚   â”‚   â”œâ”€â”€ debate_orchestrator.py  # Multi-round debate orchestration
â”‚   â”‚   â””â”€â”€ prompts.py          # System prompt management
â”‚   â”œâ”€â”€ routing/                 # Logic layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ semantic_router.py  # Author selection logic
â”‚   â”‚   â””â”€â”€ response_aggregator.py  # Response formatting
â”‚   â”œâ”€â”€ api/                     # API server
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py             # FastAPI application
â”‚   â”‚   â”œâ”€â”€ routes.py           # API endpoints
â”‚   â”‚   â””â”€â”€ schemas.py          # Pydantic models
â”‚   â””â”€â”€ ui/                      # Web interface
â”‚       â”œâ”€â”€ index.html
â”‚       â”œâ”€â”€ app.js
â”‚       â””â”€â”€ styles.css
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ authors/                 # Author profiles & prompts
â”‚   â”‚   â”œâ”€â”€ marx.yaml
â”‚   â”‚   â”œâ”€â”€ whitman.yaml
â”‚   â”‚   â””â”€â”€ baudelaire.yaml
â”‚   â””â”€â”€ settings.py             # Application configuration
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init_database.py        # Database initialization
â”‚   â”œâ”€â”€ ingest_author.py        # Data ingestion pipeline
â”‚   â””â”€â”€ create_expertise_profiles.py  # Generate author profiles
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                   # Unit tests
â”‚   â””â”€â”€ integration/            # Integration tests
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md         # Detailed architecture
â”‚   â”œâ”€â”€ API.md                  # API documentation
â”‚   â””â”€â”€ DEPLOYMENT.md           # Deployment guide
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Source texts (not in git)
â”‚   â”œâ”€â”€ processed/              # Cleaned & chunked texts
â”‚   â””â”€â”€ embeddings/             # Pre-computed embeddings
â”œâ”€â”€ .env.example                # Environment template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ pyproject.toml             # Poetry configuration
â””â”€â”€ README.md                   # This file
```

## ğŸ› ï¸ Development Phases

### Phase 1: MVP - Single-Author & Data Pipeline âœ…

- [x] P1.1: Project setup & configuration
- [x] P1.2: Data ingestion pipeline
- [x] P1.3: RAG pipeline (single and multi-author)
- [x] P1.4: Basic UI

**Goal**: Working chat interface with authors responding using RAG. âœ…

### Phase 2: Multi-Author Router âœ…

- [x] P2.1: Create expertise profiles (Marx, Whitman, Manson, and more)
- [x] P2.2: Implement semantic router with threshold-based selection
- [x] P2.3: Update UI for automatic author selection

**Goal**: System automatically selects relevant authors based on query. âœ…

### Phase 3: Virtual Debate Panel âœ…

- [x] P3.1: Parallel processing for concurrent responses
- [x] P3.2: System prompt enforcement (3-paragraph limit)
- [x] P3.3: Response aggregation & comparative formatting
- [x] P3.4: Streaming support via Server-Sent Events
- [x] P3.5: Response caching and telemetry

**Goal**: Full multi-author debate with clear contrasting viewpoints. âœ…

## ğŸ”§ Configuration

### Environment Variables

```bash
# LLM Configuration
LLM_PROVIDER=gemini  # or 'openai', 'anthropic'
GEMINI_API_KEY=your_key_here
GEMINI_MODEL=gemini-2.0-flash-exp  # Current default
OPENAI_API_KEY=your_key_here
ANTHROPIC_API_KEY=your_key_here

# Vector Database
VECTOR_DB=chromadb  # or 'pinecone'
CHROMA_PERSIST_DIR=./data/chroma_db
PINECONE_API_KEY=your_key_here
PINECONE_ENVIRONMENT=us-west1-gcp

# Embedding Model
EMBEDDING_MODEL=text-embedding-004  # or 'text-embedding-ada-002'

# Semantic Router
RELEVANCE_THRESHOLD=0.60  # Empirically tested optimal value
MIN_AUTHORS=2
MAX_AUTHORS=5

# API Server
API_HOST=0.0.0.0
API_PORT=8000
CORS_ORIGINS=http://localhost:3000
```

### Author Configuration

Author profiles are defined in `config/authors/` as YAML files. Currently available authors:

- **Karl Marx** - Political economy, capitalism, class struggle
- **Walt Whitman** - Poetry, democracy, transcendentalism, American identity
- **Mark Manson** - Psychology, self-help, personal development, modern culture

Example configuration:

```yaml
# config/authors/marx.yaml
name: Karl Marx
expertise_domains:
  - political economy
  - capitalism
  - class struggle
  - labor theory of value
voice_characteristics:
  tone: analytical, critical, revolutionary
  vocabulary: dialectical, materialist, proletarian
  perspective: class-based analysis
system_prompt: |
  You are Karl Marx, the 19th-century philosopher and economist...
  [Full system prompt]
```

## ğŸ“Š Data Requirements

### Input Format

Place source texts in `data/raw/<author>/`:

```
data/raw/
â”œâ”€â”€ marx/
â”‚   â”œâ”€â”€ capital_vol1.txt
â”‚   â”œâ”€â”€ communist_manifesto.txt
â”‚   â””â”€â”€ grundrisse.txt
â”œâ”€â”€ whitman/
â”‚   â””â”€â”€ leaves_of_grass.txt
â””â”€â”€ manson/
    â”œâ”€â”€ subtle_art.txt
    â””â”€â”€ everything_is_fucked.txt
```

### Processing Pipeline

1. **Chunking**: Split texts into ~500-token segments with 50-token overlap
2. **Embedding**: Generate vectors using text-embedding-004 or equivalent
3. **Storage**: Store in vector DB with metadata (author, book, page)
4. **Profiling**: Create single expertise vector per author for routing

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test suite
pytest tests/unit/test_semantic_router.py
pytest tests/integration/test_rag_pipeline.py
```

## ğŸ“ˆ Performance Targets

- **Query Latency**: <3s for single author, <5s for panel (achieved)
- **Concurrent Authors**: 5 simultaneous RAG pipelines (implemented)
- **Vector Search**: <200ms per author (achieved)
- **LLM Generation**: <2s per author with streaming (implemented)
- **Cache Hit Rate**: >70% for repeated queries (implemented)

## ğŸš¢ Deployment

**Fully Automated Deployment** via Google Cloud Build:
```bash
git push origin main  # Automatically deploys backend + frontend!
```

The deployment pipeline automatically:
- âœ… Builds and deploys backend to Cloud Run
- âœ… Deploys frontend to Cloud Storage
- âœ… Configures API endpoints
- âœ… Sets up public access
- âœ… Total time: ~6-9 minutes

### Documentation
- **[Automated Deployment Guide](docs/AUTOMATED_DEPLOYMENT.md)** - Full automation setup
- **[Deployment Guide](docs/DEPLOYMENT.md)** - Infrastructure setup
- **[API Documentation](docs/API.md)** - Complete API reference
- **[Architecture](docs/ARCHITECTURE.md)** - System architecture overview
- **[Service Accounts](docs/SERVICE_ACCOUNTS_GUIDE.md)** - Permissions setup
- **[Usage Guide](USAGE.md)** - Detailed usage instructions

## ğŸ¤ Contributing

1. Follow PEP 8 style guidelines
2. Add tests for new features
3. Update documentation
4. Submit PR with clear description

## ğŸ“ License

MIT License - See LICENSE file for details.

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

---

**Status**: All 6 implementation phases complete! âœ…
- âœ… Phase 1-2: Data acquisition & frontend
- âœ… Phase 3-4: Streaming, caching, telemetry
- âœ… Phase 5-6: Automated deployment & docs
# Production deployment
# Deployment fix
